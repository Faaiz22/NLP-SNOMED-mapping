# ğŸ¥ SNOMED Medical NLP System

A comprehensive medical diagnosis mapping and analysis system for SNOMED CT codes using advanced NLP techniques.

## ğŸ“‹ Overview

This system provides a complete suite of tools for analyzing, validating, and exploring SNOMED CT medical diagnosis mappings. It combines machine learning, natural language processing, and interactive visualizations to help medical professionals and researchers work with diagnostic data.

## âœ¨ Features

### ğŸ” **Data Explorer**
- Interactive searchable SNOMED mapping table
- Dataset filtering and sorting capabilities
- Real-time search across diagnoses, codes, and abbreviations
- Comprehensive data statistics and metrics

### ğŸ§  **Semantic Search**
- Advanced NLP-powered diagnosis search
- BioBERT and clinical embeddings support
- Fuzzy string matching and similarity scoring
- Top-k semantic matches with confidence scores

### ğŸ“Š **Advanced Analytics**
- Statistical outlier detection using Z-scores
- Dataset distribution analysis and clustering
- PCA and t-SNE visualization for pattern discovery
- Correlation analysis between datasets

### ğŸ¤– **Machine Learning Models**
- **Mapping Classifier**: Predicts mapping status (accepted/rejected/needs review)
- **Confidence Predictor**: Estimates mapping confidence scores
- **Multi-class Classifier**: Suggests appropriate SNOMED codes
- **Semantic Retriever**: Clinical embedding-based search system

### ğŸ” **Quality Auditor**
- Comprehensive data quality assessment
- Duplicate and near-duplicate detection
- SNOMED code format validation
- Abbreviation consistency checking
- Missing data analysis
- Statistical anomaly detection

### ğŸ¯ **Diagnosis Predictor**
- Real-time diagnosis code suggestion
- Top-3 SNOMED code recommendations
- Frequency-based matching
- Interactive prediction interface

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- pip package manager

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/snomed-medical-nlp.git
cd snomed-medical-nlp
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Prepare your data**
   - Place your `SNOMED_mappings_unscored.csv` file in the root directory
   - Ensure the CSV follows the expected format (see Data Format section)

4. **Run the application**
```bash
streamlit run streamlit_app.py
```

5. **Open your browser**
   - Navigate to `http://localhost:8501`
   - Start exploring your SNOMED data!

## ğŸ“ Project Structure

```
snomed-medical-nlp/
â”œâ”€â”€ streamlit_app.py          # Main Streamlit application
â”œâ”€â”€ models.py                 # ML models and embeddings
â”œâ”€â”€ data_processor.py         # Data processing utilities
â”œâ”€â”€ quality_auditor.py        # Data quality assessment
â”œâ”€â”€ utils.py                  # Utility functions and helpers
â”œâ”€â”€ config.py                 # Configuration settings
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ .gitignore               # Git ignore rules
â”œâ”€â”€ cache/                   # Model cache directory
â”‚   â””â”€â”€ embeddings/          # Cached embeddings
â””â”€â”€ exports/                 # Exported files directory
```

## ğŸ“Š Data Format

Your CSV file should contain the following columns:

| Column | Description | Example |
|--------|-------------|---------|
| `Dx` | Diagnosis description | "atrial fibrillation" |
| `SNOMED CT Code` | SNOMED CT identifier | "49436004" |
| `Abbreviation` | Standard abbreviation | "AF" |
| `CPSC` | Case count in CPSC dataset | "150" |
| `PTB` | Case count in PTB dataset | "89" |
| `PTB-XL` | Case count in PTB-XL dataset | "234" |
| `Georgia` | Case count in Georgia dataset | "67" |
| `Total` | Total case count | "540" |

## ğŸ”§ Configuration

The system uses a comprehensive configuration system in `config.py`. Key settings include:

- **Model Settings**: Embedding models, similarity thresholds
- **Data Settings**: File paths, column mappings
- **UI Settings**: Visualization preferences, table pagination
- **Quality Settings**: Validation thresholds, audit parameters

## ğŸ—ï¸ Architecture

### Core Components

1. **Data Layer** (`data_processor.py`)
   - Data loading and validation
   - Preprocessing and cleaning
   - Statistical analysis

2. **Model Layer** (`models.py`)
   - Machine learning classifiers
   - Embedding-based semantic search
   - Confidence prediction

3. **Quality Layer** (`quality_auditor.py`)
   - Comprehensive data validation
   - Anomaly detection
   - Consistency checking

4. **Presentation Layer** (`streamlit_app.py`)
   - Interactive web interface
   - Real-time visualizations
   - User interaction handling

5. **Utility Layer** (`utils.py`)
   - Helper functions
   - Visualization components
   - Export utilities

## ğŸ¤– Machine Learning Models

### 1. Mapping Classifier
Predicts whether a source-target mapping will be:
- âœ… **Accepted**: High confidence mapping
- âŒ **Rejected**: Poor quality mapping  
- âš ï¸ **Needs Review**: Uncertain mapping requiring expert review

### 2. Confidence Predictor
Estimates numerical confidence scores (0-1) for mappings using:
- Textual similarity features
- Ontology-based semantic distance
- Pre-trained biomedical language models

### 3. Semantic Retriever
Finds similar diagnoses using:
- Clinical embeddings (BioBERT, ClinicalBERT)
- TF-IDF vectorization
- Fuzzy string matching
- Cosine similarity scoring

### 4. Multi-class Classifier
Suggests the most appropriate SNOMED codes for new diagnoses:
- Top-k prediction with confidence scores
- Clinical domain-aware feature extraction
- Frequency-based weighting

## ğŸ“ˆ Quality Auditing

The quality auditor performs comprehensive checks:

### Data Integrity
- âœ… Duplicate diagnosis detection
- âœ… SNOMED code format validation
- âœ… Case count consistency verification
- âœ… Missing data identification

### Semantic Consistency
- âœ… Abbreviation-diagnosis alignment
- âœ… Medical keyword extraction
- âœ… Domain-specific validation
- âœ… Ontological relationship checking

### Statistical Analysis
- âœ… Outlier detection (Z-score based)
- âœ… Distribution analysis
- âœ… Cross-dataset correlation
- âœ… Anomaly pattern recognition

## ğŸ¨ Visualizations

The system provides rich interactive visualizations:

- **ğŸ“Š Bar Charts**: Dataset distributions and comparisons
- **ğŸ¥§ Pie Charts**: Proportional breakdowns
- **ğŸ”¥ Heatmaps**: Correlation matrices and patterns
- **â˜€ï¸ Sunburst Charts**: Hierarchical data exploration
- **ğŸ“ˆ Scatter Plots**: Relationship analysis
- **ğŸ—ºï¸ Treemaps**: Nested data representation
- **ğŸ•¸ï¸ Radar Charts**: Multi-dimensional comparisons

## ğŸ” Usage Examples

### Basic Data Exploration
```python
# Load and explore data
from data_processor import DataProcessor
processor = DataProcessor('SNOMED_mappings_unscored.csv')
summary = processor.get_summary_statistics()
```

### Semantic Search
```python
# Find similar diagnoses
from models import SemanticRetriever
retriever = SemanticRetriever(df)
results = retriever.search("chest pain", k=5)
```

### Quality Audit
```python
# Run comprehensive quality check
from quality_auditor import SNOMEDQualityAuditor
auditor = SNOMEDQualityAuditor(df)
audit_results = auditor.run_full_audit()
```

## ğŸš€ Deployment

### Local Development
```bash
streamlit run streamlit_app.py
```

### Streamlit Cloud
1. Push code to GitHub
2. Connect to Streamlit Cloud
3. Deploy with automatic updates

### Docker Deployment
```dockerfile
FROM python:3.9-slim
WORKDIR /app
COPY . .
RUN pip install -r requirements.txt
EXPOSE 8501
CMD ["streamlit", "run", "streamlit_app.py"]
```

## ğŸ¤ Contributing

1. **Fork the repository**
2. **Create a feature branch**
   ```bash
   git checkout -b feature/amazing-feature
   ```
3. **Make your changes**
4. **Add tests** (if applicable)
5. **Commit your changes**
   ```bash
   git commit -m 'Add amazing feature'
   ```
